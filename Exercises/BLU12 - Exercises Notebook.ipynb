{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d5043cb5780972701d72559872f7ef6f",
     "grade": false,
     "grade_id": "cell-f224e965c17e4734",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU12 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21db749d1bcc06ba873ca869c34a527a",
     "grade": false,
     "grade_id": "cell-84b8869fd97c7856",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import hashlib\n",
    "\n",
    "import surprise\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "from surprise.prediction_algorithms import BaselineOnly, KNNBaseline, Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ceb0e1f4ccedc024c7c0e081da41749",
     "grade": false,
     "grade_id": "cell-78aeb1cd83e9b72f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 1 About the data\n",
    "\n",
    "The dataset is a small sample of the [goodbooks-10k dataset](https://github.com/zygmuntz/goodbooks-10k) available on GitHub.\n",
    "\n",
    "The original dataset contains ten thousand books and six million ratings, containing:\n",
    "* Books marked read by users\n",
    "* Book metadata\n",
    "* Tags, shelves, and genres.\n",
    "\n",
    "What makes the dataset so productive is that it covers:\n",
    "* Ratings (explicit feedback)\n",
    "* Unary data (implicit feedback), in the form of books marked to read by users\n",
    "* Book metadata, including content information\n",
    "* User-generated content (UGC), as tags, shelves, and genres assigned by users to items.\n",
    "\n",
    "In this exercise, we focus on ratings to practice a primary Recommender Systems (RS) workflow.\n",
    "\n",
    "A blogpost describing the dataset can be found [here](http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/).\n",
    "\n",
    "The sample included in `data/` contains the 100 users and the 100 items with the most ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b498362e5f7624a8e387eeeb28dcedfb",
     "grade": false,
     "grade_id": "cell-1b7b8f337a674d3a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 2 Custom Datasets in Surprise\n",
    "\n",
    "Up until now, we only used [build-in datasets](https://surprise.readthedocs.io/en/stable/dataset.html) with Surprise, namely [MovieLens](https://grouplens.org/datasets/movielens/).\n",
    "\n",
    "Although MovieLens is a stable in RS research, we try something different and use books in this exercise.\n",
    "\n",
    "The downside is that, since we are exploring new applications, we have to create our custom dataset.\n",
    "\n",
    "How do we go about it? Well, an excellent place to start is the Surprise's [dataset module documentation](https://surprise.readthedocs.io/en/stable/dataset.html).\n",
    "\n",
    "Also, Surprise provides a [Reader class](https://surprise.readthedocs.io/en/stable/reader.html), used to parse a file containing ratings, in a conventional form.\n",
    "\n",
    "We must preview our ratings file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9b09f7744a70eb73c214db7a74395322",
     "grade": false,
     "grade_id": "cell-6a2fbe60fc00ea79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def preview_file(path, nrows):\n",
    "    \n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    return lines[:nrows]\n",
    "\n",
    "\n",
    "path = os.path.join('data', 'ratings.csv')\n",
    "preview_file(path=path, nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9eb55f6af6e6ddb8eaa67c130ece9d36",
     "grade": false,
     "grade_id": "cell-2e9c7b5a4c274684",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What do we have here?\n",
    "\n",
    "We have a comma-separated file containing one rating per line, in the familiar `'uid,iid,rui'` (i.e., user, item, rating) form.\n",
    "\n",
    "We also have a header row, **which is not a rating**, even if it is in a ratings file.\n",
    "\n",
    "Time to build a custom dataset from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f976a165f45ef43eb7e102c6fff2a3fe",
     "grade": false,
     "grade_id": "cell-44c1fce0d3595e29",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset():\n",
    "    \n",
    "    path = os.path.join('data', 'ratings.csv')\n",
    "    \n",
    "    # Instantiate a Surprise Reader object.\n",
    "    # Pay close attention to the Reader class parameters and the contents\n",
    "    # of the file.\n",
    "    # reader = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Load the dataset from the file, and return it.\n",
    "    # Refer to the dataset module docs.\n",
    "    # dataset = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def make_trainset():\n",
    "\n",
    "    dataset = make_dataset()\n",
    "    \n",
    "    # Call a method on dataset (e.g., dataset.method()) that returns the whole\n",
    "    # dataset, without any splits\n",
    "    # Used in the learning materials.\n",
    "    # trainset = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return trainset\n",
    "\n",
    "\n",
    "dataset = make_dataset()\n",
    "ratings = make_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "058beff4f961627e853cf71aa4460a3a",
     "grade": false,
     "grade_id": "cell-3120868a4b138ac9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, `ratings` is an object of the [Trainset class](https://surprise.readthedocs.io/en/stable/trainset.html), containing the training set.\n",
    "\n",
    "A `Dataset` contains the raw data, while the `Trainset` is a higher-level structure, where useful attributes and methods are defined.\n",
    "\n",
    "(For examples, pay close attention to the graded tests below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7e032b4a0cc4b24126dc779d0a1c762",
     "grade": true,
     "grade_id": "cell-75778ee2fc8499b3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(type(ratings) == surprise.trainset.Trainset)\n",
    "\n",
    "assert(ratings.n_users == 100)\n",
    "assert(ratings.n_items == 100)\n",
    "assert(ratings.n_ratings == 3304)\n",
    "\n",
    "assert(ratings.rating_scale == (1, 5))\n",
    "\n",
    "assert(round(ratings.global_mean) == 4)\n",
    "\n",
    "assert(ratings.knows_item(43) == True)\n",
    "assert(ratings.knows_user(100) == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70653d952c41462b175fcde9174e3e22",
     "grade": false,
     "grade_id": "cell-778fce9dd4b9d7c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 3 Baseline\n",
    "\n",
    "We know what to do: get to a baseline real fast.\n",
    "\n",
    "We generate baseline predictions using the [baseline model](https://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.baseline_only.BaselineOnly) described in the learning materials.\n",
    "\n",
    "The [configurations](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baseline-estimates-configuration) fo the baseline estimator are:\n",
    "* We want to use Stochastic Gradient Descent (SGD)\n",
    "* With a learning rate of 0.0005\n",
    "* And regularization parameter of 0.05.\n",
    "\n",
    "We want the function to return cross-validation results, using `cv=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2657e6f5ac46754229cc5c66708cb185",
     "grade": false,
     "grade_id": "cell-cf8e5abb9e777cec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def baseline_cross_validate(data):\n",
    "    \n",
    "    # Configure the baseline options.\n",
    "    # Refer back to the learning materials.\n",
    "    # bsl_options = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Instantiate the baseline algorithm, using the bsl_options above.\n",
    "    # baseline = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Cross validation results, using cv=5.\n",
    "    # res = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return baseline, res\n",
    "\n",
    "\n",
    "# You must decide whether or not to use a `Dataset` or a `Trainset`.\n",
    "baseline, baseline_results = baseline_cross_validate(dataset)\n",
    "baseline_results['test_rmse'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d013fb0bfffe29cb8aa2d588ab1af21",
     "grade": true,
     "grade_id": "cell-f492717b9a617686",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(list(baseline.bsl_options.values()) == ['sgd', 5e-05, 0.05])\n",
    "\n",
    "assert(type(baseline_results) == dict)\n",
    "\n",
    "assert(list(baseline_results.keys()) == ['test_rmse', 'test_mae', 'fit_time', 'test_time'])\n",
    "\n",
    "assert(len(baseline_results['test_rmse']) == len(baseline_results['test_mae'] == 5))\n",
    "\n",
    "assert(baseline_results['test_rmse'].mean() >= baseline_results['test_mae'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c10281abf04e0430d9b1d52874f1b0e9",
     "grade": false,
     "grade_id": "cell-433101cae258c1db",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 4 *k*-NN (with Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "56c12544bb8a2cdab5237a34e82f6ee3",
     "grade": false,
     "grade_id": "cell-f34cf89b37ce235d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Surprise provides [*k*-NN](https://surprise.readthedocs.io/en/stable/knn_inspired.html#) inspired algorithms for Collaborative Filtering (CF).\n",
    "\n",
    "These algorithms include many of the extensions we discussed in the learning materials, especially ratings normalization.\n",
    "\n",
    "We will use a built-in algorithm similar to using the means or the z-score to standardize the ratings, but using the baseline instead.\n",
    "\n",
    "(You should find such a model [around here](https://surprise.readthedocs.io/en/stable/knn_inspired.html#).)\n",
    "\n",
    "Again, we want our function to return the cross-validation results with `cv=5`, so we can compare them with the baseline.\n",
    "\n",
    "Also, we want the following similiary options (in case of doubt, refer back to the learning materials):\n",
    "* We use the cosine similarity to compute distances\n",
    "* We want to item-item similarities\n",
    "* We want the minimum number of common users to be 3.\n",
    "\n",
    "Finally, the max number of neighbors to take into account for prediction should be 20 and the minimum 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "20572652bf8403bba7bcfa16e4f0ed5b",
     "grade": false,
     "grade_id": "cell-23da6c213920781e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fancy_knn_cross_validate(data):\n",
    "    \n",
    "    # Use the same baseline options as above.\n",
    "    # bsl_options = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Configure the similarity options.\n",
    "    # We use the item-item cosine similarity, with at least 3 common users.\n",
    "    # sim_options = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Instantiate the k-NN algorithm that considers the baselines.\n",
    "    # The maximum number of neighbors should be 20 and the minimum 5.\n",
    "    # fancy_knn = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Do cross-validation, with cv=5.\n",
    "    # res = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return fancy_knn, res\n",
    "\n",
    "\n",
    "fancy_knn, fancy_knn_results = fancy_knn_cross_validate(dataset)\n",
    "fancy_knn_results['test_rmse'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "76619d3e5d004b59eb08b1596b5f8ef2",
     "grade": true,
     "grade_id": "cell-de5940a60cc46e63",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(list(fancy_knn.bsl_options.values()) == ['sgd', 5e-05, 0.05])\n",
    "assert(list(fancy_knn.sim_options.values()) == ['cosine', False, 3])\n",
    "\n",
    "assert(fancy_knn.k == 20)\n",
    "assert(fancy_knn.min_k == 5)\n",
    "\n",
    "assert(type(fancy_knn_results) == dict)\n",
    "\n",
    "assert(list(fancy_knn_results.keys()) == ['test_rmse', 'test_mae', 'fit_time', 'test_time'])\n",
    "\n",
    "assert(len(fancy_knn_results['test_rmse']) == len(baseline_results['test_mae'] == 5))\n",
    "\n",
    "assert(fancy_knn_results['test_rmse'].mean() >= baseline_results['test_mae'].mean())\n",
    "\n",
    "assert(type(fancy_knn) == KNNBaseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "53970f4cec386d0102b17450dbc7d15e",
     "grade": false,
     "grade_id": "cell-c1476433a5c32fa2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 5 Making Predictions\n",
    "\n",
    "Now that we have a winner, we want to make predictions.\n",
    "\n",
    "What we will do is:\n",
    "* Train the fancy $k$-NN model, that normalizes ratings using the baselines, on the entire dataset\n",
    "* Create a list of unknown ratings that can be used a test set, i.e., all the ratings that are not in the train set\n",
    "* Make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "469a0f1909726a27b56fc438af23ef79",
     "grade": false,
     "grade_id": "cell-28154a587fefc80f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_predictions(algo, ratings_train):\n",
    "    \n",
    "    # Fit the algorith (received as a parameter) on the training data.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Create a test set with the ratings that are not in the trainset.\n",
    "    # Refer back to the learning materials, or the `Trainset` docs.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Generate the set of test predictions.\n",
    "    # In doubt, refer back to the learning materials.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    \n",
    "model, preds = make_predictions(fancy_knn, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d452f5befc8ef81277baf6297ac8e157",
     "grade": false,
     "grade_id": "cell-939ba4cf6c8eb8ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Time to make some predictions!\n",
    "\n",
    "What are the predicting ratings for the following user-item pairs:\n",
    "* `uid='2487'`, `iid=46`\n",
    "* `uid='951'`, `iid=72`\n",
    "* `uid=10146`, `iid=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "87b27a9f245d39d1547513920545ea1c",
     "grade": false,
     "grade_id": "cell-dd8d91eb71204c97",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Use the model returned above to make predictions, and retrieve the\n",
    "# estimate attribute, as in the learning materials.\n",
    "# If needed, look into the predictions.\n",
    "# Pleace note that user and item IDs need to be passed as strings.\n",
    "# pred_1 = ...\n",
    "# pred_2 = ...\n",
    "# pred_3 = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "583391d5584b1681d3c20e9319f20e54",
     "grade": true,
     "grade_id": "cell-0010974c176b2903",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(len(preds) == 6696)\n",
    "\n",
    "assert(type(preds[0]) == Prediction)\n",
    "\n",
    "expected_hash = '6e540aa1b0f070c6190738483002fd3aed4e797b85a8a424aa129c2afdb2b50b'\n",
    "assert(hashlib.sha256(pred_1).hexdigest() == expected_hash)\n",
    "\n",
    "expected_hash = 'fffecd8226b856eefea0997034a60b38b775e79a9f423739627dce7fcaeecde0'\n",
    "assert(hashlib.sha256(pred_2).hexdigest() == expected_hash)\n",
    "\n",
    "expected_hash = 'e0641648822eeb8658a539a9ad833f61f0c70662e944c7b21bd85c2f484f670c'\n",
    "assert(hashlib.sha256(pred_3).hexdigest() == expected_hash)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
