{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU12 - Learning Notebook - Part 2 of 3 - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "\n",
    "from surprise.accuracy import mae, rmse\n",
    "\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "\n",
    "from surprise.prediction_algorithms import BaselineOnly, KNNBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Standard Notation\n",
    "\n",
    "## 1.1 Users and Items\n",
    "\n",
    "We write $U$ and $I$ the sets of all users and items. Individual users $u, v \\in U$ and items $i, j \\in I$.\n",
    "\n",
    "The set $U_i$ contains all users that rated $i$, and $U_{ij}$ contains users that rated both $i$ and $j$.\n",
    "\n",
    "Similarly, $I_u$ stands for all items rated by user $u$, $I_{uv}$ for items rated by both users $u$ and $v$.\n",
    "\n",
    "## 1.2 Ratings\n",
    "\n",
    "$R$ denotes the set of all ratings, while $\\hat{R}$ is the set of predicted ratings.\n",
    "\n",
    "We introduce $R_{train}$ and $R_{test}$, which stand for ratings in the train and test sets, respectively.\n",
    "\n",
    "Concerning ratings, $r_{ui}$ is the true rating given by user $u$ to the item $i$, while $\\hat{r}_{ui}$ is the predicted one.\n",
    "\n",
    "## 1.3 Statistics\n",
    "\n",
    "The mean of all ratings is $\\mu$, $\\mu_u$ is the mean of all ratings by $u$, and $\\mu_i$ is the mean rating given to $i$.\n",
    "\n",
    "Finally, $\\sigma$ is the global standard deviation, $\\sigma_u$ and $\\sigma_i$ are the standard deviations for user $u$ and item $i$, respectively.\n",
    "\n",
    "# 2 Scikit-Surprise\n",
    "\n",
    "In this notebook, we introduce `scikit-surprise`, Surprise from now on, a Python package that excels at Collaborative Filtering (CF).\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "> [Surprise](http://surpriselib.com/) is a Python [scikit](https://www.scipy.org/scikits.html) for building and analyzing recommender systems.\n",
    "\n",
    "The package provides convenient implementations for most steps in the CF pipeline, including:\n",
    "* Dataset handling\n",
    "* Built-in similarity measures\n",
    "* Ready-to-use prediction algorithms, including baseline and neighborhood methods, plus advanced approaches\n",
    "* Model selection methods.\n",
    "\n",
    "Once we develop a solid intuition about RS and CF, we can use it prototype ideas, without excessive focus on implementation.\n",
    "\n",
    "(Surprise takes care of sparsity, vectorization and linear algebra for us, that is.)\n",
    "\n",
    "Recommendation algorithms in Surprise work like standard `sklearn` estimators.\n",
    "\n",
    "# 3 Load Dataset\n",
    "\n",
    "Surprise allows us to use built-in datasets, or to build our own, which we do in due time. \n",
    "\n",
    "Fortunately, the [MovieLens](https://grouplens.org/datasets/movielens/100k/) dataset we've been using it's readily available.\n",
    "\n",
    "The `Dataset` class is used to manage datasets, although we should never instantiate it directly, instead using:\n",
    "* `Dataset.load_builtin()`: load a built-in dataset\n",
    "* `Dataset.load_from_file()`: load a dataset from a custom file\n",
    "* `Dataset.load_from_foalds()`: load a dataset from custom files with predefined cross-validation folds.\n",
    "\n",
    "We use the `Dataset.load_builtin()` method to load (and download, if needed) the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the cross-validation folds are not predefined (or don't exist at all), we have to explicitly define them.\n",
    "\n",
    "We use `build_full_trainset()` to avoid splitting the dataset into folds, returning the ratings from the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = dataset.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Baseline Model\n",
    "\n",
    "Now that we have the ratings $R$ in a convenient format, we want to get to a baseline fast. (No surprises there.)\n",
    "\n",
    "Ratings exhibit systematic user and item tendencies, i.e., some users give, and some items receive higher ratings than others.\n",
    "\n",
    "There is a simple, yet effective, way to generate baseline predictions from such tendencies or *biases*.\n",
    "\n",
    "We write $b_{ui}$ the baseline estimate, which accounts for user bias and item bias.\n",
    "\n",
    "$$\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$$\n",
    "\n",
    "Typically, the calculations of $b_u$ and $b_i$ are coupled, for accuracy.\n",
    "\n",
    "## 4.1 Example\n",
    "\n",
    "Consider an average movie rating $\\mu$ of 3. \n",
    "\n",
    "Knowing that item $i$ is rated 0.5 stars above average, and that $u$ rates 0.3 below average, we have:\n",
    "\n",
    "$$\\hat{r}_{ui} = b_{ui} = 3 + 0.5 + (-0.3) = 3.2$$\n",
    "\n",
    "In this case, $\\hat{r}_{ui}$ corresponds to the rating of a critical user to a good movie.\n",
    "\n",
    "## 4.2 Estimating the Biases\n",
    "\n",
    "Consider the $(u, i) \\in U \\times I$ pairs, such as $K = \\{(u, i) |  r_{ui}$ is known $\\}$.\n",
    "\n",
    "It works by minimizing the regularized squared error, as:\n",
    "\n",
    "$$\\min_{b*} \\sum\\limits_{(u, i) \\in K} (r_{ui} - (\\mu + b_u + b_i))^2 + \\lambda(b_u^2 + b_i^2)$$\n",
    "\n",
    "The first term $(r_{ui} - (\\mu + b_u + b_i))^2$ corresponds to the error, finds the best $b_u$ and $b_i$, for all $u \\in U$ and $i \\in I$.\n",
    "\n",
    "The regularization term $\\lambda(b_u^2 + b_i^2)$ manages overfitting, penalizing the magnitudes of the parameters.\n",
    "\n",
    "Baselines can be estimated in two different ways: stochastic gradient descent (SGD) and alternating least squares (ALS).\n",
    "\n",
    "## 4.3 Implementation\n",
    "\n",
    "We start by initializing the estimator, the built-in `BaselineOnly()`, an implementation of the algorithm above.\n",
    "\n",
    "By passing a parameter `bsl_options`, we define how baselines are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd', 'learning_rate': 0.005}\n",
    "\n",
    "\n",
    "baseline = BaselineOnly(bsl_options=bsl_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `.fit()` method to train the algorithm and initialize some internal structures, including the similarity matrix (when needed).\n",
    "\n",
    "Then, the `.predict()` method predicts $\\hat{r}_{ui}$, calling the defined estimate method, i.e., the baseline algorithm. \n",
    "\n",
    "We use the attribute `.est` to retrieve the estimated rating $\\hat{r}_{ui}$ from the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = str(196)\n",
    "iid = str(302)\n",
    "\n",
    "pred = baseline.fit(R).predict(uid, iid)\n",
    "\n",
    "pred.est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Collaborative Filtering (CF)\n",
    "\n",
    "The most common approach to CF uses neighborhood models.\n",
    "\n",
    "In the CF pipeline, before prediction, we have to compute user-user or item-item similarities.\n",
    "\n",
    "![collaborative_filtering](../media/collaborative_filtering.png)\n",
    "\n",
    "Thus, central to both is the similarity measure.\n",
    "\n",
    "## 5.1 Distance Measures\n",
    "\n",
    "### 5.1.1 Mean Squared Difference (MSD)\n",
    "\n",
    "By default, Surprise uses the MSD.\n",
    "\n",
    "The MSD shares the pitfalls of the Euclidean Distance because it considers the magnitude of the vectors and is sensitive to scaling.\n",
    "\n",
    "#### User-user MSD\n",
    "\n",
    "$$msd(u, v) = \\frac{\\sum\\limits_{i \\in I_{uv}} (r_{ui} - r_{vi})^2}{|I_{uv}|}$$\n",
    "\n",
    "#### Item-item MSD\n",
    "\n",
    "$$msd(i, j) = \\frac{\\sum\\limits_{u \\in U_{ij}} (r_{ui} - r_{uj})^2}{|U_{ij}|}$$\n",
    "\n",
    "### 5.1.2 Cosine Similarity\n",
    "\n",
    "The cosine similarity is the most widely used. Also, it is a normalized dot-product, i.e., considers only differences in direction.\n",
    "\n",
    "#### User-user cosine\n",
    "\n",
    "$$cos(u, v) = \\frac{u \\cdot v}{||u|| \\cdot ||v||}$$\n",
    "\n",
    "#### Item-item cosine\n",
    "\n",
    "$$cos(i, j) = \\frac{i \\cdot j}{||i|| \\cdot ||j||}$$\n",
    "\n",
    "### 5.1.3 Pearson Correlation\n",
    "\n",
    "Another popular similarity measure is the Pearson correlation.\n",
    "\n",
    "#### User-user correlation\n",
    "\n",
    "$$pearson(u, v) = \\frac{cov(u, v)}{\\sigma_u \\cdot \\sigma_v}$$\n",
    "\n",
    "#### Item-item correlation\n",
    "\n",
    "$$pearson(i, j) = \\frac{cov(i, j)}{\\sigma_i \\cdot \\sigma_j}$$\n",
    "\n",
    "## 5.2 Implementation\n",
    "\n",
    "We configure similarity measures the same way we did for baseline options, i.e., by passing a `sim_options` parameter to our estimator.\n",
    "\n",
    "These are the available options, in line with our needs:\n",
    "* `name`: the name of the similarity measure to use (including `cosine`, `msd`, and `pearson`)\n",
    "* `user_based`: `True` if we want to compute user-user similarities, `False` for item-item\n",
    "* `min_support`: the minimum number of common items (if `user_based: True`) or users (if `user_based: False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'cosine', 'user_based': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity-based brand of recommenders we have studied fits under the $k$-NN family.\n",
    "\n",
    "### 5.2.1 User-user prediction\n",
    "\n",
    "Consider $N_i^k(u)$ as the $k$ nearest neighbors of user $u$ that have rated $i$, computed using a similarity metrics of our choice.\n",
    "\n",
    "$$\\hat{r}_{ui} = \\frac{\\sum\\limits_{v \\in N_i^k(u)} sim(u, v) \\cdot r_{vi}}{\\sum\\limits_{v \\in N_i^k(u)} sim(u, v)}$$\n",
    "\n",
    "### 5.2.2 Item-item prediction\n",
    "\n",
    "Take also $N_u^k(i)$, the $k$ nearest neighbors of item $i$ rated by user $u$, computed using our similarity measure.\n",
    "\n",
    "$$\\hat{r}_{ui} = \\frac{\\sum\\limits_{j \\in N_u^k(i)} sim(i, j) \\cdot r_{uj}}{\\sum\\limits_{v \\in N_u^k(i)} sim(i, j)}$$\n",
    "\n",
    "In the `sim_options` above, we already chose item-item recommendations based on cosine similarities.\n",
    "\n",
    "Now, as we usually do, we initialize the estimator and call `fit()` and `predict()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNBasic(sim_options=sim_options)\n",
    "knn.fit(R)\n",
    "pred = knn.predict(uid, iid)\n",
    "    \n",
    "pred.est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Surprise provides extensions to the base $k$-NN, including using [mean-centering](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans), [z-score](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithZScore) and [bias](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline) to scale predictions.\n",
    "\n",
    "All prediction algorithms are [here](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html), with excellent documentation.\n",
    "\n",
    "Now, we zoom in into how we perform model validation and selection in RS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Model Selection\n",
    "\n",
    "When ratings are available, the RS core computation is to predict $\\hat{r}_{ui}$ for new items $i \\in I \\setminus I_u$ for user $u$.\n",
    "\n",
    "We learn a function $f$ that maps user-item pairs into ratings $f : U \\times I \\to S$ given by $\\hat{r}_{u, i} = f(u, i)$.\n",
    "\n",
    "Assuming a continuous ratings scales $S$, e.g., $S = [1, 5]$, we have ratings prediction as regression.\n",
    "\n",
    "Given a loss function $\\mathcal{L}$ that compares predictions $\\hat{r}_{ui} = f(u, i)$ with known ratings $r_{ui}$, we want $f$ that minimizes the total cost $J$.\n",
    "\n",
    "$$\\min_f J = \\sum\\limits_{r_{ui} \\in R} \\mathcal{L}(f(u, i), r_{ui})$$\n",
    "\n",
    "Consequentially, we need model validation after training to evaluate different recommendation techniques. \n",
    "\n",
    "In this sense, an RS is just like any other ML system we have studied so far.\n",
    "\n",
    "## 6.1 Measuring Accuracy\n",
    "\n",
    "### 6.1.1 Mean Absolute Error (MAE)\n",
    "\n",
    "The two most popular accuracy measures for continuous variables are the MAE and the RMSE.\n",
    "\n",
    "Both metrics express average prediction error in units of the variable of interest, and lower values are always best.\n",
    "\n",
    "The MAE measures the average of the absolute errors $|f(u, i) - r_{ui}|$, i.e., the average magnitude of the prediction errors.\n",
    "\n",
    "$$MAE(f) = \\frac{\\sum\\limits_{r_{ui} \\in R_{test}} |f(u, i) - r_{ui} |}{|R_{test}|}$$\n",
    "\n",
    "From an interpretation standpoint, the MAE is best.\n",
    "\n",
    "### 6.1.2 Root Mean Squared Error (RMSE)\n",
    "\n",
    "The RMSE is the square root of the average of squared errors $(f(u, i) - r_{ui})^2$.\n",
    "\n",
    "$$MSE(f) = \\frac{\\sum\\limits_{r_{ui} \\in R_{test}} (f(u, i) - r_{ui})^2}{|R_{test}|}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$RMSE(f) = \\sqrt{MSE(f)}$$\n",
    "\n",
    "Since the errors are squared and then averaged, the RMSE gives a higher weight to large errors.\n",
    "\n",
    "The $RMSE(f) \\geqslant MAE(f)$, since the RMSE amplifies large errors. If all of the errors have the same magnitude, then $RMSE(f)=MAE(f)$.\n",
    "\n",
    "## 6.2 Train-test Split\n",
    "\n",
    "In order to perform model validation, we need to split our ratings dataser $R$ into $R_{train}$ and $R_{test}$.\n",
    "\n",
    "Surprise provides a `model_selection` package, inspired in `sklearn`.\n",
    "\n",
    "The package contains a `train_test_split()` function, that splits a dataset (i.e., a Surprise `Dataset` object) into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train, R_test = train_test_split(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by computing the MAE and RMSE for the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.fit(R_train)\n",
    "R_pred = baseline.test(R_test)\n",
    "\n",
    "mae(R_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(R_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. \n",
    "\n",
    "What about the $k$-NN, item-item CF model? Is it any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(R_train)\n",
    "R_pred_ = knn.test(R_test)\n",
    "\n",
    "mae(R_pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Not promissing, the baseline seems to outperform the $k$-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(R_pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's official: the $k$-NN approach to CF, built on top of item-item cosine similarities, isn't an improvement.\n",
    "\n",
    "## 6.3 Cross-validation\n",
    "\n",
    "Without surprise, given the similarities with `sklearn`, we can also run a cross validation procedure.\n",
    "\n",
    "We pass algorithm, dataset and the number of splits to the `cross_validate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_validate(baseline, dataset, measures=['RMSE', 'MAE'], cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look into `res`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return is a dictionary with accuracy metrics, fit time and test time.\n",
    "\n",
    "Time to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['test_mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['test_rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the cross-validation process for the $k$-NN, item-item CF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = cross_validate(knn, dataset, measures=['RMSE', 'MAE'], cv=5)\n",
    "\n",
    "res_['test_mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_['test_rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's official: the $k$-NN doesn't improve the baseline predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
