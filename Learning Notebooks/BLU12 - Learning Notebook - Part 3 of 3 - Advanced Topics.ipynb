{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU12 - Learning Notebook- Part 3 of 3 - Advanced Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.prediction_algorithms import KNNBasic\n",
    "\n",
    "from scipy.sparse import coo_matrix, dok_matrix\n",
    "\n",
    "from make_data import make_data\n",
    "from export_ratings import export_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Implicit Feedback\n",
    "\n",
    "Most times, RS algorithms ingest implicit feedback, i.e., unary ratings, to understand user preferences.\n",
    "\n",
    "In such cases, the unary data indicates whether a user $u \\in U$ performed a given action (e.g., click, purchase).\n",
    "\n",
    "Afterward, this data is used on its own or combined with explicit ratings.\n",
    "\n",
    "In a way, ratings from unary data are ratings $r_{ui} \\in S = \\{1\\}$, i.e., with a singleton or unit set of possible ratings $S$.\n",
    "\n",
    "Absent ratings $r_ui \\notin R$ indicates that we have no information relating the user $u$ to the item $i$, just like before.\n",
    "\n",
    "(Perhaps the user purchased the item somewhere else, or the user didn't click the item because he didn't see it.)\n",
    "\n",
    "![collaborative_filtering_unary](../media/collaborative_filtering_unary.png)\n",
    "\n",
    "We make, however, some distinctions. \n",
    "\n",
    "## 1.1 Example\n",
    "\n",
    "We generated some fake unary data, using the [Faker](https://faker.readthedocs.io/en/master/) package.\n",
    "\n",
    "In `Learning Notebooks/make_data.py`, you find the `make_data()` function that generates two COO sparse matrices. \n",
    "\n",
    "This function is exactly like in the learning materials and exercises from [BLU11](https://github.com/LDSSA/batch2-blu11), so we don't repeat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, clicks, purchases = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains exactly 50 users and 50 items, i.e., $|U| = 50$ and $|I| = 50$.\n",
    "\n",
    "We include 500 clicks and 500 purchases for us to play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Duplicate Entries\n",
    "\n",
    "For starters, the user $u \\in U$ can perform an action, i.e., implicitly rate, multiple times for the same item $i$.\n",
    "\n",
    "This violates the assumptions of the matrix $R$, so upstream consolidation is required, enforcing one rating $r_ui$ for each pair $(u, i) \\in U \\times I$.\n",
    "\n",
    "Again, let $A$ be set of unary ratings, i.e., $a_{ui} \\in S = \\{1\\}$, for user-item pairs $(u, i) \\in U \\times I$, which contains duplicate pairs.\n",
    "\n",
    "A common technique is to sum together duplicate entries, as:\n",
    "\n",
    "$$\\sum\\limits_{(u, i) \\in U \\times I} a_{ui}$$\n",
    "\n",
    "As we've seen in [BLU11](https://github.com/LDSSA/batch2-blu11), this is the default behavior when we convert from COO to CSR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_ = clicks.tocsr()\n",
    "clicks_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction from 500 to 460 stored element in the matrix is due to the consolidation.\n",
    "\n",
    "We can confirm this by calling `.max()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_ = purchases.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another conventional technique is to use the logarithm of the sum, instead.\n",
    "\n",
    "$$\\log{\\sum\\limits_{(u, i) \\in U \\times I} a_{ui}}$$\n",
    "\n",
    "The log transformation is particularly useful with right-skewed distributions, i.e., not centered, with a peak on the left and a tail on the right.\n",
    "\n",
    "(Imagine a user $u$ with few clicks on many items and many of clicks on a few items, which is very common.)\n",
    "\n",
    "We can apply this quickly if so we choose, by applying the logaritm element-wise on the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_.log1p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases.log1p()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Inferring Ratings\n",
    "\n",
    "Also, since we have multiple signals relating the user $u$ to item $i$, we have to consolidate them into a single rating.\n",
    "\n",
    "Different signals (e.g., impressions, clicks, purchases) have distinct signal-to-noise ratios and levels of intent and, thus, may require different weights.\n",
    "\n",
    "Consider the set $D$, containing all types of implicit feedback, e.g., $D = \\{Click, Purchase\\}$, with the associated weights $W$.\n",
    "\n",
    "We can compute the ratings $r_{ui}$, for $(u, i) \\in U \\times I$, as:\n",
    "\n",
    "$$r_{ui} = \\sum\\limits_{(u, i) \\in U \\times I} \\Big(\\sum\\limits_{d \\in D} w_d \\cdot a_{ui}^d \\Big)$$\n",
    "\n",
    "In our example, we attribute more relevance to purchases than clicks.\n",
    "\n",
    "(Please note that Python silently converts from COO to CSR, summing together duplicate entries by default.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ratings(c, p, w_c, w_p):\n",
    "    return w_c * c + w_p * p \n",
    "\n",
    "\n",
    "ratings = make_ratings(clicks, purchases, .3, .7)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Exporting Ratings\n",
    "\n",
    "Once we have final ratings, it's good practice to export them in long-form, using the `'uid,iid,rating'` convention.\n",
    "\n",
    "We can do this easily, by converting back to COO and use the `.row`, `.col` and `.data` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_ = ratings.tocoo()\n",
    "\n",
    "uid = np.array([users[row] for row in ratings_.row], dtype='O')\n",
    "iid = np.array([items[col] for col in ratings_.col], dtype='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ratings_.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full implementation detail and NumPy nitty gritty, refer to `Learning Notebooks/export_ratings.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ratings(users, items, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here onwards, we can use all the RS techniques we have learned.\n",
    "\n",
    "(Including using the Surprise package.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Generating top-*N* Lists\n",
    "\n",
    "Often, we task the RS with recommending a list $L_u$, containing $N$ items likely to be of interest to an active user $u$.\n",
    "\n",
    "This type of output is particularly frequent in the presence of implicit feedback and unary data, as ratings loose meaning *per se*.\n",
    "\n",
    "How can we generate such a list $L_u$, using Surprise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_builtin('ml-100k')\n",
    "R_train = dataset.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `KNNBasic` to generate predictions, with all the defaults.\n",
    "\n",
    "(This may take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNBasic()\n",
    "knn.fit(R_train)\n",
    "\n",
    "R_test = R_train.build_anti_testset()\n",
    "R_pred = knn.test(R_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Surprise documentation, [this](https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-get-the-top-n-recommendations-for-each-user) is the recommended way to extract a top-$N$ list for each user. \n",
    "\n",
    "(Slightly adapted, so that we can use it in the future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \n",
    "    top_n = defaultdict(list)\n",
    "    \n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = [x[0] for x in user_ratings[:n]]\n",
    "\n",
    "    return pd.DataFrame.from_dict(data=top_n, orient='index')\n",
    "\n",
    "\n",
    "L = get_top_n(R_pred, n=10)\n",
    "L.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we generate a ranked list of recommendations $L_u$ for each user $u \\in U$, in a convenient format:\n",
    "* One row per user, indexed with the `uid`\n",
    "* One column per recommendation, ordered by the estimated ranking.\n",
    "\n",
    "Now, we learn how to evaluate algorithms focused on learning top-$N$ lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Evaluation Metrics for top-*N* Lists\n",
    "\n",
    "When ratings are not available, i.e., with unary data, measuring the rating prediction accuracy isn't possible.\n",
    "\n",
    "In these cases, evaluation is done using $R_{train}$ to learn $L_u$ and evaluating on $R_{test}$\n",
    "\n",
    "Let $T_u \\subset I_u \\cap I_{test}$ the subset of test items that the user $u$ found relevant, e.g., rated positively, clicked, purchased.\n",
    "\n",
    "## 3.1 Precision\n",
    "\n",
    "Precision measures how many recommended items are relevant, out of all recommended items to the user $u$.\n",
    "\n",
    "$$Precision(L_u) = \\frac{|L_u \\cap T_u |}{|L_u|}$$\n",
    "\n",
    "To evaluate the RS as a whole, we average the precision for all active users $u \\in U$.\n",
    "\n",
    "$$Precision(L) = \\frac{\\sum\\limits_{u \\in U} Precision(L_u)}{|U|}$$\n",
    "\n",
    "## 3.2 Recall\n",
    "\n",
    "Recall, on the other side, relates to how many relevant were recommended, out of all relevant items for the user $u$.\n",
    "\n",
    "$$Recall(L_u) = \\frac{|L_u \\cap T_u |}{|T_u|}$$\n",
    "\n",
    "Again, to evaluate the TS we average the results of all active users $u \\in U$.\n",
    "\n",
    "$$Recall(L) = \\frac{\\sum\\limits_{u \\in U} Recall(L_u)}{|U|}$$\n",
    "\n",
    "## 3.3 Average Precision (AP)\n",
    "\n",
    "Precision and recall ignore the ordering. Therefore we need a ranking metric.\n",
    "\n",
    "To understand average precision, we must start with Precision@k and Recall@k, i.e., precision and recall up to cut-off $k$.\n",
    "\n",
    "In other words, we consider only the subset of recommendations $L_u^k \\subset L_u$ from rank 1 through rank $k \\leqslant N$.\n",
    "\n",
    "$$PrecisionAtk(L_u) = \\frac{|L_u^k \\cap T_u |}{|L_u^k|}$$\n",
    "\n",
    "$$RecallAtk(L_u) = \\frac{|L_u^k \\cap T_u |}{|T_u|}$$\n",
    "\n",
    "The AP is a ranking metric, measuring the frequency of relevant recommendations.\n",
    "\n",
    "$$APatN(L_u) = \\frac{\\sum\\limits_{k = 1}^N (PrecisionAtk(L_u) \\cdot relevant(k^{th})}{|T_u|}$$\n",
    "\n",
    "The $relevant(k^{th})$ bit is a boolean value, indicating whether the $k$-th element is relevant, or not.\n",
    "\n",
    "Every hit is valued as how many correct recommendations $|L_u^k \\cap T_u|$ we have up to the rank $k$, out of all recommendations $|L_u^k|$.\n",
    "\n",
    "A first interpretation is that the AP increases only with correct recommendations (what a surprise!).\n",
    "\n",
    "Also, early hits, i.e., front-loading correct recommendations, carry over and are continuously rewarded.\n",
    "\n",
    "Finally, the AP can never decrease as you increase $N$.\n",
    "\n",
    "There is, however, an alternative formula for AP, in terms of both precision and the change in recall from the subset $k$ − 1 to the $k$-th.\n",
    "\n",
    "$$APatN(L_u) = \\sum\\limits_{k=1}^NPrecisionAtk(L_u) * \\Delta RecallAtk(L_u)$$ \n",
    "\n",
    "## 3.4 Mean Average Precision (mAP)\n",
    "\n",
    "The Average Precision (AP) is further averaged over all users and reported as a single score.\n",
    "\n",
    "$$mAPatN(L) = \\frac{\\sum\\limits_{u \\in U} APatN(L_u)}{|U|}$$\n",
    "\n",
    "This way, we use a metric that considers both the number and the ranking of hits, i.e., useful recommendations.\n",
    "\n",
    "In this last section, we learned how to use unary data, make predictions based on it and how to evaluate our algorithms.\n",
    "\n",
    "Time to practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
